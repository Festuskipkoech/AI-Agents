{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTmTGgObqeHkXlHv2yNuhw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "feceed25767e407f95d245cd9bc51181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_095dd4d8455d44b6aad0eacae31247dc",
              "IPY_MODEL_c7d8cb026ed34b8fa2aed2e1d49f6cb0",
              "IPY_MODEL_7cd7e4f6e8114cb8aa98c210089b1e82"
            ],
            "layout": "IPY_MODEL_f0c80b4c6bc142a690a4fb5348b7cb9a"
          }
        },
        "095dd4d8455d44b6aad0eacae31247dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f340442cf84aeda37fcdf851fbcbbd",
            "placeholder": "​",
            "style": "IPY_MODEL_1eec619581a24a6998578458cea11070",
            "value": "Map: 100%"
          }
        },
        "c7d8cb026ed34b8fa2aed2e1d49f6cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe71f7786696454bb398f94c47a51579",
            "max": 1029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eb132d7a8144fc4b662a20c2270594e",
            "value": 1029
          }
        },
        "7cd7e4f6e8114cb8aa98c210089b1e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07502441e6474d0c86962f935b0f52e1",
            "placeholder": "​",
            "style": "IPY_MODEL_f9e22bf509414147ab4755cb60cf1f12",
            "value": " 1029/1029 [00:01&lt;00:00, 639.13 examples/s]"
          }
        },
        "f0c80b4c6bc142a690a4fb5348b7cb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f340442cf84aeda37fcdf851fbcbbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eec619581a24a6998578458cea11070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe71f7786696454bb398f94c47a51579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb132d7a8144fc4b662a20c2270594e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07502441e6474d0c86962f935b0f52e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e22bf509414147ab4755cb60cf1f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8e8793b6554603a1f3584e3d0e6182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb4ce8f9a5cc4bdc8ecc0b46b82bafab",
              "IPY_MODEL_91b205733cfe45bdb18f2c20fc6eb11a",
              "IPY_MODEL_49111408b8364ff7be6ef2d6d6c42155"
            ],
            "layout": "IPY_MODEL_f4fc17e9f6074edfbf94f2e033df78e5"
          }
        },
        "eb4ce8f9a5cc4bdc8ecc0b46b82bafab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d68ca7d77334aa59b4802e0b98649c2",
            "placeholder": "​",
            "style": "IPY_MODEL_2ade0ff717604aa6b63c520d78f09c0a",
            "value": "Map: 100%"
          }
        },
        "91b205733cfe45bdb18f2c20fc6eb11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97dc882fcdfb49f19644f180e1ae1452",
            "max": 115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef77f96fa5cf4cbf9b61f64b9332a06c",
            "value": 115
          }
        },
        "49111408b8364ff7be6ef2d6d6c42155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866d2309954d4af4aa7052ee59a39636",
            "placeholder": "​",
            "style": "IPY_MODEL_40eea97db3ec4290b3b7552d48b657d1",
            "value": " 115/115 [00:00&lt;00:00, 820.75 examples/s]"
          }
        },
        "f4fc17e9f6074edfbf94f2e033df78e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d68ca7d77334aa59b4802e0b98649c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ade0ff717604aa6b63c520d78f09c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97dc882fcdfb49f19644f180e1ae1452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef77f96fa5cf4cbf9b61f64b9332a06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "866d2309954d4af4aa7052ee59a39636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40eea97db3ec4290b3b7552d48b657d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Festuskipkoech/AI-Agents/blob/main/DataPreparationForTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "feceed25767e407f95d245cd9bc51181",
            "095dd4d8455d44b6aad0eacae31247dc",
            "c7d8cb026ed34b8fa2aed2e1d49f6cb0",
            "7cd7e4f6e8114cb8aa98c210089b1e82",
            "f0c80b4c6bc142a690a4fb5348b7cb9a",
            "d4f340442cf84aeda37fcdf851fbcbbd",
            "1eec619581a24a6998578458cea11070",
            "fe71f7786696454bb398f94c47a51579",
            "3eb132d7a8144fc4b662a20c2270594e",
            "07502441e6474d0c86962f935b0f52e1",
            "f9e22bf509414147ab4755cb60cf1f12",
            "1c8e8793b6554603a1f3584e3d0e6182",
            "eb4ce8f9a5cc4bdc8ecc0b46b82bafab",
            "91b205733cfe45bdb18f2c20fc6eb11a",
            "49111408b8364ff7be6ef2d6d6c42155",
            "f4fc17e9f6074edfbf94f2e033df78e5",
            "2d68ca7d77334aa59b4802e0b98649c2",
            "2ade0ff717604aa6b63c520d78f09c0a",
            "97dc882fcdfb49f19644f180e1ae1452",
            "ef77f96fa5cf4cbf9b61f64b9332a06c",
            "866d2309954d4af4aa7052ee59a39636",
            "40eea97db3ec4290b3b7552d48b657d1"
          ]
        },
        "id": "B_Sffwo_4dxg",
        "outputId": "7e2898a5-bddd-445f-be69-a85e84d03a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Processing MIT lecture notes...\n",
            "Created 1144 training examples\n",
            "Setting up model for training...\n",
            "Model named modules:\n",
            "\n",
            "transformer\n",
            "transformer.wte\n",
            "transformer.wpe\n",
            "transformer.drop\n",
            "transformer.h\n",
            "transformer.h.0\n",
            "transformer.h.0.ln_1\n",
            "transformer.h.0.attn\n",
            "transformer.h.0.attn.c_attn\n",
            "transformer.h.0.attn.c_proj\n",
            "transformer.h.0.attn.attn_dropout\n",
            "transformer.h.0.attn.resid_dropout\n",
            "transformer.h.0.ln_2\n",
            "transformer.h.0.mlp\n",
            "transformer.h.0.mlp.c_fc\n",
            "transformer.h.0.mlp.c_proj\n",
            "transformer.h.0.mlp.act\n",
            "transformer.h.0.mlp.dropout\n",
            "transformer.h.1\n",
            "transformer.h.1.ln_1\n",
            "transformer.h.1.attn\n",
            "transformer.h.1.attn.c_attn\n",
            "transformer.h.1.attn.c_proj\n",
            "transformer.h.1.attn.attn_dropout\n",
            "transformer.h.1.attn.resid_dropout\n",
            "transformer.h.1.ln_2\n",
            "transformer.h.1.mlp\n",
            "transformer.h.1.mlp.c_fc\n",
            "transformer.h.1.mlp.c_proj\n",
            "transformer.h.1.mlp.act\n",
            "transformer.h.1.mlp.dropout\n",
            "transformer.h.2\n",
            "transformer.h.2.ln_1\n",
            "transformer.h.2.attn\n",
            "transformer.h.2.attn.c_attn\n",
            "transformer.h.2.attn.c_proj\n",
            "transformer.h.2.attn.attn_dropout\n",
            "transformer.h.2.attn.resid_dropout\n",
            "transformer.h.2.ln_2\n",
            "transformer.h.2.mlp\n",
            "transformer.h.2.mlp.c_fc\n",
            "transformer.h.2.mlp.c_proj\n",
            "transformer.h.2.mlp.act\n",
            "transformer.h.2.mlp.dropout\n",
            "transformer.h.3\n",
            "transformer.h.3.ln_1\n",
            "transformer.h.3.attn\n",
            "transformer.h.3.attn.c_attn\n",
            "transformer.h.3.attn.c_proj\n",
            "transformer.h.3.attn.attn_dropout\n",
            "transformer.h.3.attn.resid_dropout\n",
            "transformer.h.3.ln_2\n",
            "transformer.h.3.mlp\n",
            "transformer.h.3.mlp.c_fc\n",
            "transformer.h.3.mlp.c_proj\n",
            "transformer.h.3.mlp.act\n",
            "transformer.h.3.mlp.dropout\n",
            "transformer.h.4\n",
            "transformer.h.4.ln_1\n",
            "transformer.h.4.attn\n",
            "transformer.h.4.attn.c_attn\n",
            "transformer.h.4.attn.c_proj\n",
            "transformer.h.4.attn.attn_dropout\n",
            "transformer.h.4.attn.resid_dropout\n",
            "transformer.h.4.ln_2\n",
            "transformer.h.4.mlp\n",
            "transformer.h.4.mlp.c_fc\n",
            "transformer.h.4.mlp.c_proj\n",
            "transformer.h.4.mlp.act\n",
            "transformer.h.4.mlp.dropout\n",
            "transformer.h.5\n",
            "transformer.h.5.ln_1\n",
            "transformer.h.5.attn\n",
            "transformer.h.5.attn.c_attn\n",
            "transformer.h.5.attn.c_proj\n",
            "transformer.h.5.attn.attn_dropout\n",
            "transformer.h.5.attn.resid_dropout\n",
            "transformer.h.5.ln_2\n",
            "transformer.h.5.mlp\n",
            "transformer.h.5.mlp.c_fc\n",
            "transformer.h.5.mlp.c_proj\n",
            "transformer.h.5.mlp.act\n",
            "transformer.h.5.mlp.dropout\n",
            "transformer.h.6\n",
            "transformer.h.6.ln_1\n",
            "transformer.h.6.attn\n",
            "transformer.h.6.attn.c_attn\n",
            "transformer.h.6.attn.c_proj\n",
            "transformer.h.6.attn.attn_dropout\n",
            "transformer.h.6.attn.resid_dropout\n",
            "transformer.h.6.ln_2\n",
            "transformer.h.6.mlp\n",
            "transformer.h.6.mlp.c_fc\n",
            "transformer.h.6.mlp.c_proj\n",
            "transformer.h.6.mlp.act\n",
            "transformer.h.6.mlp.dropout\n",
            "transformer.h.7\n",
            "transformer.h.7.ln_1\n",
            "transformer.h.7.attn\n",
            "transformer.h.7.attn.c_attn\n",
            "transformer.h.7.attn.c_proj\n",
            "transformer.h.7.attn.attn_dropout\n",
            "transformer.h.7.attn.resid_dropout\n",
            "transformer.h.7.ln_2\n",
            "transformer.h.7.mlp\n",
            "transformer.h.7.mlp.c_fc\n",
            "transformer.h.7.mlp.c_proj\n",
            "transformer.h.7.mlp.act\n",
            "transformer.h.7.mlp.dropout\n",
            "transformer.h.8\n",
            "transformer.h.8.ln_1\n",
            "transformer.h.8.attn\n",
            "transformer.h.8.attn.c_attn\n",
            "transformer.h.8.attn.c_proj\n",
            "transformer.h.8.attn.attn_dropout\n",
            "transformer.h.8.attn.resid_dropout\n",
            "transformer.h.8.ln_2\n",
            "transformer.h.8.mlp\n",
            "transformer.h.8.mlp.c_fc\n",
            "transformer.h.8.mlp.c_proj\n",
            "transformer.h.8.mlp.act\n",
            "transformer.h.8.mlp.dropout\n",
            "transformer.h.9\n",
            "transformer.h.9.ln_1\n",
            "transformer.h.9.attn\n",
            "transformer.h.9.attn.c_attn\n",
            "transformer.h.9.attn.c_proj\n",
            "transformer.h.9.attn.attn_dropout\n",
            "transformer.h.9.attn.resid_dropout\n",
            "transformer.h.9.ln_2\n",
            "transformer.h.9.mlp\n",
            "transformer.h.9.mlp.c_fc\n",
            "transformer.h.9.mlp.c_proj\n",
            "transformer.h.9.mlp.act\n",
            "transformer.h.9.mlp.dropout\n",
            "transformer.h.10\n",
            "transformer.h.10.ln_1\n",
            "transformer.h.10.attn\n",
            "transformer.h.10.attn.c_attn\n",
            "transformer.h.10.attn.c_proj\n",
            "transformer.h.10.attn.attn_dropout\n",
            "transformer.h.10.attn.resid_dropout\n",
            "transformer.h.10.ln_2\n",
            "transformer.h.10.mlp\n",
            "transformer.h.10.mlp.c_fc\n",
            "transformer.h.10.mlp.c_proj\n",
            "transformer.h.10.mlp.act\n",
            "transformer.h.10.mlp.dropout\n",
            "transformer.h.11\n",
            "transformer.h.11.ln_1\n",
            "transformer.h.11.attn\n",
            "transformer.h.11.attn.c_attn\n",
            "transformer.h.11.attn.c_proj\n",
            "transformer.h.11.attn.attn_dropout\n",
            "transformer.h.11.attn.resid_dropout\n",
            "transformer.h.11.ln_2\n",
            "transformer.h.11.mlp\n",
            "transformer.h.11.mlp.c_fc\n",
            "transformer.h.11.mlp.c_proj\n",
            "transformer.h.11.mlp.act\n",
            "transformer.h.11.mlp.dropout\n",
            "transformer.h.12\n",
            "transformer.h.12.ln_1\n",
            "transformer.h.12.attn\n",
            "transformer.h.12.attn.c_attn\n",
            "transformer.h.12.attn.c_proj\n",
            "transformer.h.12.attn.attn_dropout\n",
            "transformer.h.12.attn.resid_dropout\n",
            "transformer.h.12.ln_2\n",
            "transformer.h.12.mlp\n",
            "transformer.h.12.mlp.c_fc\n",
            "transformer.h.12.mlp.c_proj\n",
            "transformer.h.12.mlp.act\n",
            "transformer.h.12.mlp.dropout\n",
            "transformer.h.13\n",
            "transformer.h.13.ln_1\n",
            "transformer.h.13.attn\n",
            "transformer.h.13.attn.c_attn\n",
            "transformer.h.13.attn.c_proj\n",
            "transformer.h.13.attn.attn_dropout\n",
            "transformer.h.13.attn.resid_dropout\n",
            "transformer.h.13.ln_2\n",
            "transformer.h.13.mlp\n",
            "transformer.h.13.mlp.c_fc\n",
            "transformer.h.13.mlp.c_proj\n",
            "transformer.h.13.mlp.act\n",
            "transformer.h.13.mlp.dropout\n",
            "transformer.h.14\n",
            "transformer.h.14.ln_1\n",
            "transformer.h.14.attn\n",
            "transformer.h.14.attn.c_attn\n",
            "transformer.h.14.attn.c_proj\n",
            "transformer.h.14.attn.attn_dropout\n",
            "transformer.h.14.attn.resid_dropout\n",
            "transformer.h.14.ln_2\n",
            "transformer.h.14.mlp\n",
            "transformer.h.14.mlp.c_fc\n",
            "transformer.h.14.mlp.c_proj\n",
            "transformer.h.14.mlp.act\n",
            "transformer.h.14.mlp.dropout\n",
            "transformer.h.15\n",
            "transformer.h.15.ln_1\n",
            "transformer.h.15.attn\n",
            "transformer.h.15.attn.c_attn\n",
            "transformer.h.15.attn.c_proj\n",
            "transformer.h.15.attn.attn_dropout\n",
            "transformer.h.15.attn.resid_dropout\n",
            "transformer.h.15.ln_2\n",
            "transformer.h.15.mlp\n",
            "transformer.h.15.mlp.c_fc\n",
            "transformer.h.15.mlp.c_proj\n",
            "transformer.h.15.mlp.act\n",
            "transformer.h.15.mlp.dropout\n",
            "transformer.h.16\n",
            "transformer.h.16.ln_1\n",
            "transformer.h.16.attn\n",
            "transformer.h.16.attn.c_attn\n",
            "transformer.h.16.attn.c_proj\n",
            "transformer.h.16.attn.attn_dropout\n",
            "transformer.h.16.attn.resid_dropout\n",
            "transformer.h.16.ln_2\n",
            "transformer.h.16.mlp\n",
            "transformer.h.16.mlp.c_fc\n",
            "transformer.h.16.mlp.c_proj\n",
            "transformer.h.16.mlp.act\n",
            "transformer.h.16.mlp.dropout\n",
            "transformer.h.17\n",
            "transformer.h.17.ln_1\n",
            "transformer.h.17.attn\n",
            "transformer.h.17.attn.c_attn\n",
            "transformer.h.17.attn.c_proj\n",
            "transformer.h.17.attn.attn_dropout\n",
            "transformer.h.17.attn.resid_dropout\n",
            "transformer.h.17.ln_2\n",
            "transformer.h.17.mlp\n",
            "transformer.h.17.mlp.c_fc\n",
            "transformer.h.17.mlp.c_proj\n",
            "transformer.h.17.mlp.act\n",
            "transformer.h.17.mlp.dropout\n",
            "transformer.h.18\n",
            "transformer.h.18.ln_1\n",
            "transformer.h.18.attn\n",
            "transformer.h.18.attn.c_attn\n",
            "transformer.h.18.attn.c_proj\n",
            "transformer.h.18.attn.attn_dropout\n",
            "transformer.h.18.attn.resid_dropout\n",
            "transformer.h.18.ln_2\n",
            "transformer.h.18.mlp\n",
            "transformer.h.18.mlp.c_fc\n",
            "transformer.h.18.mlp.c_proj\n",
            "transformer.h.18.mlp.act\n",
            "transformer.h.18.mlp.dropout\n",
            "transformer.h.19\n",
            "transformer.h.19.ln_1\n",
            "transformer.h.19.attn\n",
            "transformer.h.19.attn.c_attn\n",
            "transformer.h.19.attn.c_proj\n",
            "transformer.h.19.attn.attn_dropout\n",
            "transformer.h.19.attn.resid_dropout\n",
            "transformer.h.19.ln_2\n",
            "transformer.h.19.mlp\n",
            "transformer.h.19.mlp.c_fc\n",
            "transformer.h.19.mlp.c_proj\n",
            "transformer.h.19.mlp.act\n",
            "transformer.h.19.mlp.dropout\n",
            "transformer.h.20\n",
            "transformer.h.20.ln_1\n",
            "transformer.h.20.attn\n",
            "transformer.h.20.attn.c_attn\n",
            "transformer.h.20.attn.c_proj\n",
            "transformer.h.20.attn.attn_dropout\n",
            "transformer.h.20.attn.resid_dropout\n",
            "transformer.h.20.ln_2\n",
            "transformer.h.20.mlp\n",
            "transformer.h.20.mlp.c_fc\n",
            "transformer.h.20.mlp.c_proj\n",
            "transformer.h.20.mlp.act\n",
            "transformer.h.20.mlp.dropout\n",
            "transformer.h.21\n",
            "transformer.h.21.ln_1\n",
            "transformer.h.21.attn\n",
            "transformer.h.21.attn.c_attn\n",
            "transformer.h.21.attn.c_proj\n",
            "transformer.h.21.attn.attn_dropout\n",
            "transformer.h.21.attn.resid_dropout\n",
            "transformer.h.21.ln_2\n",
            "transformer.h.21.mlp\n",
            "transformer.h.21.mlp.c_fc\n",
            "transformer.h.21.mlp.c_proj\n",
            "transformer.h.21.mlp.act\n",
            "transformer.h.21.mlp.dropout\n",
            "transformer.h.22\n",
            "transformer.h.22.ln_1\n",
            "transformer.h.22.attn\n",
            "transformer.h.22.attn.c_attn\n",
            "transformer.h.22.attn.c_proj\n",
            "transformer.h.22.attn.attn_dropout\n",
            "transformer.h.22.attn.resid_dropout\n",
            "transformer.h.22.ln_2\n",
            "transformer.h.22.mlp\n",
            "transformer.h.22.mlp.c_fc\n",
            "transformer.h.22.mlp.c_proj\n",
            "transformer.h.22.mlp.act\n",
            "transformer.h.22.mlp.dropout\n",
            "transformer.h.23\n",
            "transformer.h.23.ln_1\n",
            "transformer.h.23.attn\n",
            "transformer.h.23.attn.c_attn\n",
            "transformer.h.23.attn.c_proj\n",
            "transformer.h.23.attn.attn_dropout\n",
            "transformer.h.23.attn.resid_dropout\n",
            "transformer.h.23.ln_2\n",
            "transformer.h.23.mlp\n",
            "transformer.h.23.mlp.c_fc\n",
            "transformer.h.23.mlp.c_proj\n",
            "transformer.h.23.mlp.act\n",
            "transformer.h.23.mlp.dropout\n",
            "transformer.ln_f\n",
            "lm_head\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feceed25767e407f95d245cd9bc51181"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/115 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c8e8793b6554603a1f3584e3d0e6182"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        }
      ],
      "source": [
        "# MIT AI Tutor Fine-tuning Pipeline\n",
        "# Complete pipeline for processing MIT lecture notes and fine-tuning a model\n",
        "!pip install PyPDF2\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import PyPDF2\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import torch\n",
        "\n",
        "class MITDataProcessor:\n",
        "    def __init__(self, pdf_path: str):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.raw_text = \"\"\n",
        "        self.processed_data = []\n",
        "\n",
        "    def extract_pdf_text(self) -> str:\n",
        "        \"\"\"Extract text from the MIT PDF\"\"\"\n",
        "        try:\n",
        "            with open(self.pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                self.raw_text = text\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def download_pdf(self, url: str) -> str:\n",
        "        \"\"\"Download PDF from MIT URL\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            filename = \"mit_lecture_notes.pdf\"\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            self.pdf_path = filename\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_sections(self) -> List[Dict]:\n",
        "        \"\"\"Extract chapters/sections from the text\"\"\"\n",
        "        sections = []\n",
        "\n",
        "        # Split by chapters or major sections\n",
        "        chapter_pattern = r'Chapter \\d+|Section \\d+|\\d+\\.\\d+\\s+[A-Z]'\n",
        "        chapters = re.split(chapter_pattern, self.raw_text)\n",
        "\n",
        "        for i, chapter in enumerate(chapters[1:], 1):  # Skip first empty split\n",
        "            if len(chapter.strip()) > 100:  # Only process substantial sections\n",
        "                sections.append({\n",
        "                    'section_id': i,\n",
        "                    'content': chapter.strip()[:3000],  # Limit length\n",
        "                    'title': f\"Section {i}\"\n",
        "                })\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def create_training_examples(self) -> List[Dict]:\n",
        "        \"\"\"Convert MIT content into training examples with simplified explanations\"\"\"\n",
        "\n",
        "        training_templates = [\n",
        "            {\n",
        "                'instruction': 'Explain this MIT 6.390 concept in simple terms for undergraduate students',\n",
        "                'complexity': 'beginner'\n",
        "            },\n",
        "            {\n",
        "                'instruction': 'Break down this machine learning concept as an MIT instructor would, but make it accessible',\n",
        "                'complexity': 'intermediate'\n",
        "            },\n",
        "            {\n",
        "                'instruction': 'Teach this concept using MIT\\'s structured approach but with intuitive explanations',\n",
        "                'complexity': 'structured'\n",
        "            },\n",
        "            {\n",
        "                'instruction': 'Use the Socratic method to help a student understand this ML concept from MIT 6.390',\n",
        "                'complexity': 'socratic'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        sections = self.extract_sections()\n",
        "        training_examples = []\n",
        "\n",
        "        for section in sections:\n",
        "            content = section['content']\n",
        "\n",
        "            # Extract key concepts (simple heuristic)\n",
        "            sentences = content.split('.')[:5]  # First 5 sentences\n",
        "            concept_text = '. '.join(sentences)\n",
        "\n",
        "            for template in training_templates:\n",
        "                example = {\n",
        "                    'instruction': template['instruction'],\n",
        "                    'input': concept_text,\n",
        "                    'output': self.generate_simplified_response(concept_text, template['complexity'])\n",
        "                }\n",
        "                training_examples.append(example)\n",
        "\n",
        "        return training_examples\n",
        "\n",
        "    def generate_simplified_response(self, content: str, complexity: str) -> str:\n",
        "        \"\"\"Generate simplified responses based on complexity level\"\"\"\n",
        "\n",
        "        # This is where you'd ideally use a teacher model to simplify\n",
        "        # For now, we'll create template responses that maintain MIT structure\n",
        "\n",
        "        if complexity == 'beginner':\n",
        "            return f\"\"\"In MIT's 6.390, we approach this systematically. Let me break this down simply:\n",
        "\n",
        "The key idea is: {content[:200]}...\n",
        "\n",
        "Think of it this way: imagine you're teaching a friend who's never seen machine learning before. The core concept becomes much clearer when we focus on the intuition rather than just the math.\n",
        "\n",
        "The MIT approach emphasizes understanding the 'why' before the 'how'. This builds stronger foundational knowledge.\"\"\"\n",
        "\n",
        "        elif complexity == 'socratic':\n",
        "            return f\"\"\"Great question! Let's think about this together using MIT's approach.\n",
        "\n",
        "First, what do you think is the main challenge here?\n",
        "\n",
        "{content[:150]}...\n",
        "\n",
        "Now, can you see why this approach makes sense? What would happen if we tried a different method?\n",
        "\n",
        "This is exactly how we tackle problems in 6.390 - by asking the right questions first.\"\"\"\n",
        "\n",
        "        elif complexity == 'structured':\n",
        "            return f\"\"\"Following MIT's structured methodology:\n",
        "\n",
        "**Problem Definition:** {content[:100]}...\n",
        "\n",
        "**Key Insight:** The fundamental principle here is about finding patterns in data systematically.\n",
        "\n",
        "**Implementation:** We break this into manageable steps:\n",
        "1. Understand the mathematical foundation\n",
        "2. See the intuitive explanation\n",
        "3. Apply to real examples\n",
        "\n",
        "**Verification:** How do we know our approach works? MIT teaches us to always validate our understanding.\"\"\"\n",
        "\n",
        "        else:  # intermediate\n",
        "            return f\"\"\"In 6.390, we balance mathematical rigor with practical understanding.\n",
        "\n",
        "{content[:250]}...\n",
        "\n",
        "The beauty of MIT's approach is that we don't just memorize formulas - we understand why they work. This concept connects to broader themes in machine learning that we'll see throughout the course.\n",
        "\n",
        "Remember: every complex idea can be broken down into simpler components.\"\"\"\n",
        "\n",
        "class MITTutorTrainer:\n",
        "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.training_data = None\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize model and tokenizer\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    def prepare_dataset(self, training_examples: List[Dict]):\n",
        "        \"\"\"Prepare dataset for training\"\"\"\n",
        "\n",
        "        # Format for instruction following\n",
        "        formatted_data = []\n",
        "        for example in training_examples:\n",
        "            text = f\"\"\"### Instruction: {example['instruction']}\n",
        "\n",
        "### Input: {example['input']}\n",
        "\n",
        "### Response: {example['output']}\"\"\"\n",
        "            formatted_data.append({'text': text})\n",
        "\n",
        "        # Create train/validation split\n",
        "        split_idx = int(0.9 * len(formatted_data))\n",
        "        train_data = formatted_data[:split_idx]\n",
        "        val_data = formatted_data[split_idx:]\n",
        "\n",
        "        # Convert to Hugging Face datasets\n",
        "        train_dataset = Dataset.from_list(train_data)\n",
        "        val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "        # Tokenize\n",
        "        def tokenize_function(examples):\n",
        "            return self.tokenizer(\n",
        "                examples['text'],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "        val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "# Inside the MITTutorTrainer class, in the setup_lora method\n",
        "# Replace the existing lora_config definition with this:\n",
        "\n",
        "    def setup_lora(self):\n",
        "        \"\"\"Setup LoRA for efficient fine-tuning\"\"\"\n",
        "        # Print model layer names to identify correct target modules\n",
        "        print(\"Model named modules:\")\n",
        "        for name, module in self.model.named_modules():\n",
        "            print(name)\n",
        "\n",
        "        # For DialoGPT (based on GPT-2), the linear layers in attention are typically named 'c_attn'\n",
        "        # However, to apply LoRA to q/k/v projections specifically within the attention\n",
        "        # block, we need to target the sub-modules if they exist or target the combined layer.\n",
        "        # Let's assume we want to target the primary linear layers within the attention mechanism.\n",
        "        # A common target for GPT-2 models is the 'c_attn' layer within the attention block.\n",
        "\n",
        "        # Re-define target_modules based on inspection or documentation for DialoGPT\n",
        "        # If 'c_attn' is a combined layer (q, k, v), you might need to adapt how LoRA is applied\n",
        "        # based on the PEFT library's capabilities for that specific module type.\n",
        "        # Let's try a different common set of targets for GPT-2 like models, like the linear layers.\n",
        "        # We need to check the actual layer names by printing them above.\n",
        "        # Based on common GPT-2 structure, 'c_attn' is the convolutional layer for attention.\n",
        "        # We need to find the actual linear layers if q_proj/v_proj aren't present.\n",
        "        # Let's inspect the names printed by the loop above. A common target for GPT-2's MHA is the c_attn layer.\n",
        "\n",
        "        # After inspecting the output of the print statement above, if 'c_attn' is present\n",
        "        # within the attention modules, let's try targeting that.\n",
        "        # If specific 'q_proj', 'v_proj' are not found, we might need to target the whole\n",
        "        # attention block or a different set of linear layers.\n",
        "        # Let's assume after inspection, we find layer names that are suitable.\n",
        "        # For DialoGPT, it seems 'c_attn' within the attention layers is a common target.\n",
        "\n",
        "        # Example of potential target modules after inspecting DialoGPT's architecture:\n",
        "        # Check for names like 'attn.c_attn' or similar within the transformer blocks.\n",
        "        # Let's assume 'c_attn' is the correct target based on GPT-2 architecture.\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            # Update target_modules based on the actual layer names found in DialoGPT\n",
        "            target_modules=[\"c_attn\"] # Example target for GPT-2 based models like DialoGPT\n",
        "        )\n",
        "\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "# Inside the MITTutorTrainer class, in the train_model method\n",
        "\n",
        "    def train_model(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./mit-tutor-model',\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=4,\n",
        "            warmup_steps=100,\n",
        "            logging_steps=10,\n",
        "            # evaluation_strategy=\"steps\", # Old parameter name\n",
        "            eval_strategy=\"steps\",  # Corrected parameter name\n",
        "            eval_steps=100,\n",
        "            save_steps=500,\n",
        "            learning_rate=5e-5,\n",
        "            fp16=True,\n",
        "            report_to=None  # Disable wandb logging\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model('./mit-tutor-final')\n",
        "\n",
        "    def generate_response(self, instruction: str, input_text: str = \"\") -> str:\n",
        "        \"\"\"Generate response using the trained model\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"### Instruction: {instruction}\n",
        "\n",
        "### Input: {input_text}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Main execution pipeline\n",
        "def main():\n",
        "    # Step 1: Process MIT Data\n",
        "    print(\"Processing MIT lecture notes...\")\n",
        "    processor = MITDataProcessor(\"\")\n",
        "\n",
        "    # Download the PDF\n",
        "    mit_url = \"https://introml.mit.edu/_static/spring24/LectureNotes/6_390_lecture_notes_spring24.pdf\"\n",
        "    pdf_path = processor.download_pdf(mit_url)\n",
        "\n",
        "    # Extract and process content\n",
        "    processor.extract_pdf_text()\n",
        "    training_examples = processor.create_training_examples()\n",
        "\n",
        "    print(f\"Created {len(training_examples)} training examples\")\n",
        "\n",
        "    # Save training data\n",
        "    with open('mit_training_data.json', 'w') as f:\n",
        "        json.dump(training_examples, f, indent=2)\n",
        "\n",
        "    # Step 2: Setup and train model\n",
        "    print(\"Setting up model for training...\")\n",
        "    trainer = MITTutorTrainer(\"microsoft/DialoGPT-medium\")  # Using smaller model for Colab\n",
        "    trainer.setup_model()\n",
        "    trainer.setup_lora()\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset, val_dataset = trainer.prepare_dataset(training_examples)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train_model(train_dataset, val_dataset)\n",
        "\n",
        "    print(\"Training completed! Model saved to ./mit-tutor-final\")\n",
        "\n",
        "    # Step 3: Test the model\n",
        "    print(\"\\nTesting the trained model:\")\n",
        "    test_response = trainer.generate_response(\n",
        "        \"Explain linear regression as an MIT instructor would, but keep it simple\",\n",
        "        \"A student is confused about the mathematical foundation\"\n",
        "    )\n",
        "    print(\"Response:\", test_response)\n",
        "\n",
        "# Inference-only class for deployment\n",
        "class MITTutorInference:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"Simple chat interface\"\"\"\n",
        "        instruction = \"Respond as an MIT AI instructor, making complex concepts accessible to students\"\n",
        "\n",
        "        prompt = f\"\"\"### Instruction: {instruction}\n",
        "\n",
        "### Input: {user_message}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}